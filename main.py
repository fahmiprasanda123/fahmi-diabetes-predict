# -*- coding: utf-8 -*-
"""portofolio_fproject_hrs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1klgQVoPeuDfLj5LWZr8XNxWj_rQFZMTY
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import pylab as py
import scipy.stats as stats

sns.set()

from mlxtend.plotting import plot_decision_regions
import missingno as msno
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""## Import Dataset"""

from google.colab import drive
drive.mount('/content/drive')

#Import Dataset
df = pd.read_csv ('/content/drive/MyDrive/PROJECT/final_project/Dataset9_Diabetes_Prediction.csv')
df.head()

"""# Exploratory Data Analysis (EDA)"""

#Exploratory Data Analysis (EDA)
df.columns

#Informasi Dataset
df.info()

df.describe()

"""# Data Preprocessing

### Missing Values
"""

#Missing Values
df.isnull().head()

df.isnull().sum()

"""Dapat diketahui bahwa terdapat nilai yang hilang diberi tanda '0'. Maka dari itu, harus direplace dengan NaN value sebelum memulai proses imputasi"""

df_copy = df.copy(deep = True)
df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)

# Menampilkan Nilai NaN
print(df_copy.isnull().sum())

"""Selanjutnya adalah mengganti nilai 0 denngan NaN Values sehingga dapat melakukan proses imputasi dengan menambahkan nilai median untuk setiap kolom.

# Data Vizualization
"""

p = df.hist(figsize = (10,10))

#Computing Mean Value
df_copy['Glucose'].fillna(df_copy['Glucose'].median(), inplace = True)
df_copy['BloodPressure'].fillna(df_copy['BloodPressure'].median(), inplace = True)
df_copy['SkinThickness'].fillna(df_copy['SkinThickness'].median(), inplace = True)
df_copy['Insulin'].fillna(df_copy['Insulin'].median(), inplace = True)
df_copy['BMI'].fillna(df_copy['BMI'].median(), inplace = True)
print(df_copy)

#Plotting The Distribution

p = df_copy.hist(figsize = (10,10))

"""Visualisasi ini dilakukan untuk melihat persebaran dataset yang nilai null valeusnya sudah diubah. Kita dapat melihat spike 50-100."""

color_wheel = {1: "#0392cf", 2: "#7bc043"}
colors = df["Outcome"].map(lambda x: color_wheel.get(x + 1))
print(df.Outcome.value_counts())
p= df.Outcome.value_counts().plot(kind="bar")

"""Dataset Imbalance, pasien diabetes setengah dari pasien non diabetes"""

#df1 untuk visualisasi
df1 = df.copy()

"""Data Status Diabetes"""

# Menambahkan status diabetes berdasarkan dari data outcome
df1.loc[df1['Outcome'] == 1, 'Status_Diabetes'] = 'Positif'
df1.loc[df1['Outcome'] == 0, 'Status_Diabetes'] = 'Negatif'

data_status =df1['Status_Diabetes'].value_counts().reset_index()
data_status.head()

_ = plt.pie (x=data_status['Status_Diabetes'],labels=data_status['index'],autopct='%1.1f%%')

"""**Key takeaway:**
* Status diabetes : Negatif = 65.1 % , Positif = 34.9%

Data Status BMI
"""

#Membuat Status_BMI
def get_bmi_Status(bmi):
  if bmi <= 18.4:
    return "low"
  elif bmi <= 25:
    return "normal"
  else:
    return "high"

df1["Status_BMI"] = df1["BMI"].apply(lambda x: get_bmi_Status(x))
df1.head()

data_bmi=df1['Status_BMI'].value_counts().reset_index()
data_bmi.head()

_ = plt.pie (x=data_bmi['Status_BMI'],labels=data_bmi['index'],autopct='%1.1f%%')

"""**Key takeaway:**
* Status diabetes : terdapat 645 orang dengan status BMI tinggi senilai 84% dari data
"""

#Melihat hubungan status diabetes dengan status BMI
data2 = df1.groupby(['Status_BMI','Status_Diabetes'])['Outcome'].count().unstack()
data2

"""**Key takeaway:**
* Status diabetes : terdapat 259 orang positif diabetes dengan status BMI tinggi

Data Blood Pressure
"""

# Menambahkan status bloodpressure berdasarkan dari data bloodpressure
df1.loc[df1['BloodPressure'] < 59, 'Status_Bloodpressure'] = 'Low'
df1.loc[df1['BloodPressure'] > 60, 'Status_Bloodpressure'] = 'Normal'
df1.loc[df1['BloodPressure'] > 80, 'Status_Bloodpressure'] = 'High'

df1.head()

data_bloodpressure =df1['Status_Bloodpressure'].value_counts().reset_index()
data_bloodpressure.head()

_=plt.bar(data_bloodpressure['index'],data_bloodpressure['Status_Bloodpressure'],color = ('blue', 'red', 'yellow'), alpha = 0.5)

_ = plt.pie (x=data_bloodpressure['Status_Bloodpressure'],labels=data_bloodpressure['index'],autopct='%1.1f%%')

"""**Key takeaway:**
* Blood Pressure : terdapat 60.9% orang dengan blood pressure normal, Namun terdapat 22.6% memiliki blood pressure yang tinggi
"""

#melihat hubungan status diabetes dengan status bloodpressure
data4 = df1.groupby(['Status_Bloodpressure','Status_Diabetes'])['Outcome'].count().unstack()
data4

data4.plot(kind='bar').set_title('Diabetes dengan Bloodpressure')

"""**Key takeaway:**
* Blood Pressure : Status bloodpressure normal memiliki data terbanyak

Data Glucose
"""

data5 = df[(df1.Status_Diabetes == 'Positif')].filter(items=['Glucose'])
data5

data6 = df[(df1.Status_Diabetes == 'Negatif')].filter(items=['Glucose'])
data6

a = plt.hist(data5['Glucose'],bins=10,alpha=0.5)
b = plt.hist(data6['Glucose'],bins=15,alpha=0.5)
a = plt.title('Perbandingan Glukosa Diabetes Positif dan Negatif')
sns.distplot(data6.Glucose)

#distribusi glucose
sns.set(style='whitegrid')
sns.distplot(df1['Glucose'],color = 'blue',bins=20)
plt.title('glucose distribution plot',fontsize=14)
plt.xlabel('Glucose',fontsize=14)
plt.ylabel('count',fontsize=14)
plt.show()

"""**Key takeaway:**
* Glucose : Semakin tinggi glucosenya semakin tinggi didiagnosa positif diabetes

Data Skin Thickness
"""

#Distribusi Skinthickness
sns.set(style='whitegrid')
sns.distplot(df1['SkinThickness'],color = 'blue',bins=20)
plt.title('SkinThickness distribution plot',fontsize=14)
plt.xlabel('SkinThickness',fontsize=14)
plt.ylabel('count',fontsize=14)
plt.show()

data8 = df1[(df1.Status_Diabetes == 'Positif')].filter(items=['SkinThickness'])
data8.head()

data9 = df1[(df1.Status_Diabetes == 'Negatif')].filter(items=['SkinThickness'])
data9.head()

_ = plt.hist(data8['SkinThickness'],bins=10,alpha=0.5)
_ = plt.hist(data9['SkinThickness'],bins=10,alpha=0.5)

"""Data Age"""

#Distribusi Age
sns.set(style='whitegrid')
sns.distplot(df1['Age'],color = 'blue',bins=20)
plt.title('BMi distribution plot',fontsize=14)
plt.xlabel('Age',fontsize=14)
plt.ylabel('count',fontsize=14)
plt.show()

data10 = df1[(df1.Status_Diabetes == 'Positif')].filter(items=['Age'])
data10.head()

data11 = df1[(df1.Status_Diabetes == 'Negatif')].filter(items=['Age'])
data11.head()

_ = plt.hist(data10['Age'],bins=20,alpha=0.5,color='red')
_ = plt.title('Data Umur Diabetes Positif')

_ = plt.hist(data11['Age'],bins=20,alpha=0.5)
_ = plt.title('Data Umur Diabetes Negatif')

"""**Key takeaway:**
* Age : terdapat beberapa orang yang positif diabetes berumur diatas 30 tahun, sedangkan yang negatif diabetes berumur dibawah 30 tahun
"""

#Korelasi variable

plt.figure(figsize=(10,8))
# seaborn has an easy method to showcase heatmap
p = sns.heatmap(df.corr(), annot=True,cmap = 'crest')

"""**Key takeaway:**
* Korelasi : Dapat dilihat pada Outcome nilai korelasi tertinggi adalah Glucose dengan nilai 0.47, selanjutnya BMI dengan nilai 0.29 dan Age dengan nilai 0.24

# Feature Scaling
"""

#Scaling Data
df_copy.head()

#Scaling digunakan untuk menyamakan skala variable
sc_X = StandardScaler()
X =  pd.DataFrame(sc_X.fit_transform(df_copy.drop(["Outcome"],axis = 1),), columns=['Pregnancies',
'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])
X.head()

y = df_copy.Outcome
y.head()

"""# Model Building"""

#Splitting The Dataset

X = df_copy.drop('Outcome', axis=1)
y = df_copy['Outcome']

#variable independent
X.head()

#Variable Dependent/Target
y.head()

"""**Split test and train dataset**"""

#Data Testing Proporsi 75:25
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,
                                                    random_state=8)
print(len(X_train))
print(len(X_test))

X_train

y_train

"""## Model Fitting"""

# Fit models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree

# Scoring functions
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

"""**Random Forest**"""

# Fit Random Forest classifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# Prediction X_test for RandomForestClassifier
y_pred1 = rf.predict(X_test)

Pred1 = pd.DataFrame(np.array(y_pred1),
                   columns=['Prediction'])
verbal=Pred1['Prediction'].map({0:'Non-diabetes', 1:'Diabetes'})
Pred1['Conclusion'] = pd.DataFrame(np.array(verbal),
                    columns=['Conclusion'])
Pred1.head(10)

y_pred1 = rf.predict(X_test)
print(classification_report(y_test, y_pred1))

print(f"Confusion Matrix: \n {confusion_matrix(y_test, y_pred1)}")

"""**KNN**"""

# Fit KNN
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

y_pred2 = knn.predict(X_test)

Pred2 = pd.DataFrame(np.array(y_pred2),
                   columns=['Prediction'])
verbal=Pred2['Prediction'].map({0:'Non-diabetes', 1:'Diabetes'})
Pred2['Conclusion'] = pd.DataFrame(np.array(verbal),
                    columns=['Conclusion'])
Pred2.head(10)

y_pred2 = knn.predict(X_test)
print(classification_report(y_test, y_pred2))

print(f"Confusion Matrix: \n {confusion_matrix(y_test, y_pred2)}")

"""**Logistic Regression**"""

# Fit logistic regression
log = LogisticRegression()
log.fit(X_train, y_train)

# Prediction X_test for LogisticRegression
y_pred3 = log.predict(X_test)

Pred3 = pd.DataFrame(np.array(y_pred3),
                   columns=['Prediction'])
Pred3['Conclusion'] = pd.DataFrame(np.array(verbal),
                    columns=['Conclusion'])
Pred3.head(10)

y_pred3 = log.predict(X_test)
print(classification_report(y_test, y_pred3))

print(f"Confusion Matrix: \n {confusion_matrix(y_test, y_pred3)}")

def get_auc_scores(y_actual, y_pred, y_proba):
    auc_score = roc_auc_score(y_actual, y_pred);
    fpr_df, tpr_df, _ = roc_curve(y_actual,  y_proba);
    return (auc_score, fpr_df, tpr_df)

auc_log, fpr_log, tpr_log = get_auc_scores(y,
                                           log.predict(X),
                                           log.predict_proba(X)[:,1])
auc_knn, fpr_knn, tpr_knn = get_auc_scores(y,
                                           knn.predict(X),
                                           knn.predict_proba(X)[:,1])
auc_rf, fpr_rf, tpr_rf = get_auc_scores(y,
                                        rf.predict(X),
                                        rf.predict_proba(X)[:,1])

plt.figure(figsize = (12,6), linewidth= 1)
plt.plot(fpr_log, tpr_log, label = 'log Score: ' + str(round(auc_log, 5)))
plt.plot(fpr_knn, tpr_knn, label = 'knn score: ' + str(round(auc_knn, 5)))
plt.plot(fpr_rf, tpr_rf, label = 'rf score: ' + str(round(auc_rf, 5)))
plt.plot([0,1], [0,1], 'k--', label = 'Random: 0.5')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.legend(loc='best')
#plt.savefig('roc_results_ratios.png')
plt.show()

#Accuracy Test
predictions = rf.predict(X_test)
print("Accuracy_Score RF =", format(metrics.accuracy_score(y_test, predictions)))

predictions = knn.predict(X_test)
print("Accuracy_Score KNN =", format(metrics.accuracy_score(y_test,predictions)))

predictions = log.predict(X_test)
print("Accuracy_Score Log =", format(metrics.accuracy_score(y_test,predictions)))

"""**Key takeaway:**
* Dapat diketahui bahwa model yang paling cocok untuk data tersebut adalah model Random Forest Classifier

## Resampling to Handle Imbalanced Dataset
"""

from imblearn.over_sampling import SMOTE
oversampling = SMOTE()
X_smote, y_smote = oversampling.fit_resample(X_train, y_train)

y_train.value_counts()

y_smote.value_counts()

# Fit Random Forest classifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
print(classification_report(y_test, y_pred_rf))

"""## Feature Importances"""

rf.feature_importances_

fig, ax = plt.subplots(figsize=(14,8))
(pd.Series(rf.feature_importances_, index=X.columns).plot(kind='barh',color=["#000000","#009b9e"]))
ax.set_ylabel("Important Features", size=20)
ax.set_title("Random Feature Importances", size=25, pad=20, color='black')
plt.show()

#Plotting feature importances
(pd.Series(rf.feature_importances_, index=X.columns)
   .plot(kind='barh'))

# Manual Hyperparameter Tuning
model=RandomForestClassifier(n_estimators=100,criterion='gini',
                             max_features='sqrt',min_samples_leaf=5,random_state=8).fit(X_smote,y_smote)
predictions=model.predict(X_smote)
print(confusion_matrix(y_smote,predictions))
print(accuracy_score(y_smote,predictions))
print(classification_report(y_smote,predictions))



#y_pred_rf